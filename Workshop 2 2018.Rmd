---
title: "Workshop 2:Classification Part 1 - Tree based approaches "
author: "Jacinta Holloway"
date: "31 January 2018"
output: html_document
---

```{r global_options, message=FALSE, results= 'hide', echo= FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      results='hide', warning=FALSE, message=FALSE)
```
Setting up your R session.
Start by making sure that your working directory is properly set.
If not you can set it using setwd().
To check your working directory use getwd.


```{r }
getwd()
```

Install required packages.   
This workshop requires the packages ggplot2, gbm, caret, rpart and dplyr.
You can check which packages you have installed using the installed.packages function. 

```{r }
installed.packages(lib.loc = NULL, priority = NULL,
                   noCache = FALSE, fields = NULL,
                   subarch = .Platform$r_arch)
```

Alternatively go to window 4, tab "Packages" and type in/search for the listed packages above and install by using the button.  
A window will open in which you have to type in the name of the package and hit install. 
If the above  packages are not listed please install them using install.packages.

```{r }

options(repos="https://cran.rstudio.com")
install.packages(c("ggplot2","gbm","caret","rpart","dplyr"))

library (ggplot2)
library(gbm)
library(caret)
library(dplyr)
library(rpart)
```

We will be working with some Landsat satellite imagery data.
Read the Landsat data from a csv file into R. Note: you will need to change the file path to the location of the file on your computer.  

```{r }
LSdata <- read.csv ("data/landsat_farms3and5.csv")
```

This creates a new file, called LSdata, as a "data frame" format. 
A data frame is used to store data tables, and is a list of vectors of equal length.
Check data type of the new data set, LSdata, using the class function. It should be a data frame. 

```{r }
class(LSdata)
```

Check the structure of the LS data set, including variable names (col names), summary statistics and information about each variable. 

```{r }
colnames(LSdata)
dim(LSdata)
summary(LSdata)
str(LSdata)
```

Satellite imagery is a type of big data, which can be inconveniently large to work with on a standard computer. 
Let's take a random sample of this data to make it easier to run models on. 
Take a random sample of size 50,000 from LSdata - sample without replacement.

```{r }
mysample <- LSdata[sample(1:nrow(LSdata), 50000,
                          replace=FALSE),] 
```

This creates a new file, called mysample, as "data frame". This is your random sample.
Check the structure of the new data set, my sample, using the class function. It should be a data frame.

```{r }
class(mysample)
```

Check the structure of the LS data set, including variable names (col names), summary statistics and information about each variable

```{r }
colnames(mysample)
dim(mysample)
summary(mysample)
str(mysample)
```

Let's remove the original larger LSdata file from the environment as we are working on the subset of the data we randomly sampled for the rest of the workshop.

```{r }
rm(LSdata)
```
## Fit a Boosted Regression Tree ##

In this example you will run a Boosted Regression Tree using the 'gbm' package. A GBM is a Generalized Boosted Regression Model.
More information about the package is [here.] (https://cran.r-project.org/web/packages/gbm/gbm.pdf)
Note: You can also run a gbm in the 'dismo' package, but we will use gbm in this example.
You can find out what type of modelling can be done in gbm using the getModelInfo function.

```{r }
getModelInfo()$gbm$type
```

GBMs are a classification technique. In this example, the GBM will perform a binary classification on crop data in the Landsat imagery. 
Create binary variables for crop.

```{r }
crop.bin = model.matrix( ~ crop - 1, data=mysample )
```

Next, split the data set, my sample, into a training data set (80%) and test data set (20%).
The training data is used to train the model, and then the model is run on the test data to check how well it is classifying the data.

```{r }
inTrain <- createDataPartition(y=mysample$crop, 
                               times = 1,
                               list = FALSE,
                               p = .8)

training <- mysample [inTrain,]
testing <- mysample [-inTrain,]
str(inTrain)
```

Check the number of rows (observations) in the training and test datasets. You should have more observations in the training data set.

```{r }
nrow(training)
nrow(testing)
```

Fit a GBM model to the data. We want the model to predict crop based on the 6 bands in the Landsat imagery. 
There are 4 hyperparameters to tune. 
These are: shrinkage, Number of trees (n.trees), interaction.depth, minimum Observations in node (minobsinnode). 

```{r }
modelLS <- gbm(crop ~ band1+band2+band3+band4+band5+band6,
               data = training,
               distribution = 'multinomial',
               n.trees=100,
               shrinkage=0.05, #also known as learning rate
               n.minobsinnode = 10,
               interaction.depth = 3)  
```

Note: all other options for the gbm are set to their defaults in the package. To see what else you could change you can look at the help file and gbm package documentation.
To see the help file, use: 

```{r }
??gbm
```
You can experiment with the number of trees and see if this changes the results. 
To estimate the optimal number of trees for the model, you can use the out-of-bag estimate (oobag.improve function). This function uses only training data.
Estimate the optimal number of trees using oobag.improve and plot the results.

```{r }
mod_improve <- modelLS$oobag.improve
plot(mod_improve)
```

Train error gives a vector of equal length to the number of fitted trees containing the value of the loss function for each boosting iteration evaluated on the training data.
Estimate train error and plot the results.

```{r }
mod_train <- modelLS$train.error
plot(mod_train)
```

Analysis of the gbm model, modelLS 
Produce summary statistics of modelLS train error to see the distribution of error in the model.

```{r }
summary(modelLS$train.error) 
```

You saw this distribution graphically using plot(mod_train).
You can predict values for each observation in the test data using the first n. trees using the predict function.
Produce predicted values for the test data using the first 100 trees. 

```{r }
prediction <- predict(modelLS, testing, n.trees=100, type="response", single.tree = FALSE) 

summary(prediction, plot = FALSE)


prediction2 <- apply(prediction, 1, which.max) # selects the highest value of the row
```

Visualise the tree using the pretty tree function. 

```{r }
pretty.gbm.tree(modelLS, i.tree = 1)  
```

Let's interpret the first row of output, which should look as follows

Node  |(SplitVar|SplitCodePred|LeftNode|RightNode|MissingNode|ErrorReduction|Weight|Prediction|
------|-----------|------|-----|-----|-----|-----|-----|      
0|3|1.067500e+03|1|2|9|433.79063|20003|0.20276228|

The root node (0) is split by the 3rd variable (the 4th column in the training data set - crop). 
Split Code Pred says all points less than 1.067 went to left node 1, and all points higher than this value went to right node 2.
All points with a missing value in column 4 went to missing node 9.
The error reduction was 433.79 due to this split.
There were 20003 weight in the root node.
0.202 is the predicted value assigned to all values in the root node before the point was split into left and right.  
Terminal nodes (decision nodes or leaves) are indicated by a '-1' in the SplitVar, Left Node, Right Node and Missing Node columns. In this example, the second row (1) is a terminal node. 

For terminal nodes, the Prediction value is the value for all the points belonging to this leaf node adjusted times the shrinkage.
Note: when the root node (0) splits into its left and right nodes, the left node is processed first until no more splits are possible,
before returning to the right node and performing splits. 
Note: the individual trees are fit to predict the gradient of the loss function evaluated at the current prediction and response. 
See [this link] (https://stats.stackexchange.com/questions/237582/interpretation-of-gbm-single-tree-prediction-in-pretty-gbm-tree) for a helpful explanation.

<div class="alert alert-info">
  <strong>Task</strong> Interpret the results for lines 3-6 inclusive
</div>

## Plots ##
It is useful to visualise your data, even using simple graphs. Produce a bar plot to view the relative importance of the different bands in the Landsat imagery.

```{r }
summary.gbm(modelLS)
```
<div class="alert alert-info">
  <strong>Task</strong> Which bands are most important for prediction? Does this make sense? https://landsat.usgs.gov/what-are-band-designations-landsat-satellites 
</div>

Produce histograms of the gbm model training error and out of bag improvement. 
You can change colours, labels, etc.  

```{r }
histogram(modelLS$oobag.improve, xlab = "Out-of-bag improvement" ) #Visualise Out-Of-Bag improvement
```
<div class="alert alert-info">
  <strong>Task</strong> Write your own code to produce a histogram of the training error, 'train.error'.
</div>

Let's now create a Classification and Regression Tree (CART) model.
This is a simple explanation of CART using rpart https://www.statmethods.net/advstats/cart.html 
Fit a CART model using the rpart function.

```{r }
cart <- rpart(crop ~ band1+band2+band3+band4+band5+band6,
             data = training)
```

Produce a plot of the tree. 

```{r }
plot(cart, uniform=TRUE, 
     main="Classification Tree for Crops")
text(cart, cex=.8)
```

Predict using the CART model.

```{r }
cartp=predict(cart, testing, type="class")
table(cartp)
```
<div class="alert alert-info">
  <strong>Task</strong> Discuss the predicted results of the CART model. 
</div>


## End of workshop code ##
